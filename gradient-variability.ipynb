{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n",
      "/tmp/ipykernel_39077/4260995017.py:28: FutureWarning: The operation <function median at 0x7f56c00a43a0> failed on a column. If any error is raised, this will raise an exception in a future version of pandas. Drop these columns to avoid this warning.\n",
      "  nw_name = NW_tbl.groupby('name').agg(np.median).sort_values('network').index.to_list()[1:]\n"
     ]
    }
   ],
   "source": [
    "# Customizable settings\n",
    "\n",
    "''' RECURRING VARIABLES '''\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import ciftools_FA as ct\n",
    "import pandas as pd\n",
    "\n",
    "# directory containing subdirectories named fter subject IDs that contain the timeseries and surface files\n",
    "root_dir = \"/home/fralberti/Data/HCP_S1200\"\n",
    "# directory where all intermediate files and the final output will be saved\n",
    "output_dir = \"/home/fralberti/Documents/BlackBox/Prj_Network-variability/Results\"\n",
    "# path to cognitive data\n",
    "cog_path = f'{root_dir}/unrestricted_fralberti_4_8_2022_8_22_23.csv'\n",
    "# .txt containing IDs of subjects to include in the analysis\n",
    "f = open(f'{root_dir}/Subjects/subj_IDs_338.txt', 'r')\n",
    "subj_id = np.array(f.read().splitlines(), dtype='int32')\n",
    "# group name\n",
    "group = '338'\n",
    "\n",
    "# Schaefer atlas to use for analyses\n",
    "lbl_N = 400\n",
    "nw_N = 7\n",
    "networks_txt = f'/home/fralberti/Data/Shaefer2018_HCP/Schaefer2018_{lbl_N}Parcels_{nw_N}Networks_order_info.txt'\n",
    "networks = nib.load(f'/home/fralberti/Data/Shaefer2018_HCP/Schaefer2018_{lbl_N}Parcels_{nw_N}Networks_order.dlabel.nii')\n",
    "NW_tbl = ct.agg_networks(networks, networks, func=np.median, by_hemisphere=False, label_tbl=True)[1]\n",
    "nw_name = NW_tbl.groupby('name').agg(np.median).sort_values('network').index.to_list()[1:]\n",
    "\n",
    "# number of jobs for parallelized operations\n",
    "nj = -5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cifti_to_npy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n",
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n",
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n",
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n",
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n",
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n",
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n",
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n",
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n",
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n",
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]][[196.  67.  47. ... 339. 339. 339.]]\n",
      "\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m np\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mgcca_dir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mgroup\u001b[39m}\u001b[39;00m\u001b[39m.gcca.\u001b[39m\u001b[39m{\u001b[39;00mnw_N\u001b[39m}\u001b[39;00m\u001b[39mNWs\u001b[39m\u001b[39m'\u001b[39m, np\u001b[39m.\u001b[39marray(grad_NW)) \n\u001b[1;32m     50\u001b[0m np\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mgcca_dir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mgroup\u001b[39m}\u001b[39;00m\u001b[39m.gcca.\u001b[39m\u001b[39m{\u001b[39;00mlbl_N\u001b[39m}\u001b[39;00m\u001b[39mParc\u001b[39m\u001b[39m'\u001b[39m, np\u001b[39m.\u001b[39marray(grad_lbl))\n\u001b[0;32m---> 51\u001b[0m np\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mgcca_dir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mgroup\u001b[39m}\u001b[39;00m\u001b[39m.gcca.32k_fs_LR\u001b[39m\u001b[39m'\u001b[39m, np\u001b[39m.\u001b[39;49marray(grad_vtx))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import ciftools_FA as ct\n",
    "import statsmodels.api as sm\n",
    "from joblib import Parallel, delayed\n",
    "# from settings import root_dir, output_dir, subj_id, group, lbl_N, nw_N, networks, NW_tbl, nw_name, nj\n",
    "subj_dir = f\"{root_dir}/Subjects/\"\n",
    "gcca_dir = f\"{output_dir}/GCCA\"\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "''' Save vertex/parcel/network-level GCCA (individual and group median) from dscalar.nii to .npy arrays'''\n",
    "\n",
    "# Write function to run in parallel\n",
    "def fun(subj, root_dir, output_dir, nw_N, lbl_N):\n",
    "    grad = nib.load(f'{root_dir}/{subj}/Analysis/{subj}.GCCA_525.32k_fs_LR.dscalar.nii')\n",
    "    NW_df, NW_tbl = ct.agg_networks(grad, networks, func=np.median, by_hemisphere=False, label_tbl=True)\n",
    "    NW_df.iloc[0, :] = 0\n",
    "    NW_tbl.iloc[0, :] = 0\n",
    "    lbl_df = ct.agg_labels(grad, networks, func=np.median)    \n",
    "    \n",
    "    # Append subject's vertex, label, and network-level gradients to group list (for SD)\n",
    "    grad_NW = NW_df.T.values\n",
    "    grad_lbl = lbl_df.T.values\n",
    "    grad_vtx = np.asarray(grad.get_fdata())\n",
    "    \n",
    "    # Save median individual median gradient of parcels and networks\n",
    "    np.save(f'{output_dir}/{subj}.gcca.{nw_N}NWs', NW_df) \n",
    "    np.save(f'{output_dir}/{subj}.gcca.{lbl_N}Parc', lbl_df)\n",
    "    \n",
    "    return grad_NW, grad_lbl, grad_vtx\n",
    "    \n",
    "#--------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create directory\n",
    "\n",
    "if not os.path.exists(gcca_dir):\n",
    "\tos.mkdir(gcca_dir, mode = 0o777)\n",
    "\n",
    "r = Parallel(n_jobs=nj)(delayed(fun)(subj, subj_dir, gcca_dir, nw_N, lbl_N) for subj in subj_id)\n",
    "# unpack output\n",
    "grad_NW, grad_lbl, grad_vtx = zip(*r)\n",
    "\n",
    "# Save parcels' and networks' median gradient of all subjects\n",
    "np.save(f'{gcca_dir}/{group}.gcca.{nw_N}NWs', np.array(grad_NW)) \n",
    "np.save(f'{gcca_dir}/{group}.gcca.{lbl_N}Parc', np.array(grad_lbl))\n",
    "np.save(f'{gcca_dir}/{group}.gcca.32k_fs_LR', np.array(grad_vtx)) \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interindividual_dispersion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import ciftools_FA as ct\n",
    "import statsmodels.api as sm\n",
    "from joblib import Parallel, delayed\n",
    "# from settings import root_dir, output_dir, subj_id, group, lbl_N, nw_N, networks, NW_tbl, nw_name, nj\n",
    "subj_dir = f\"{root_dir}/Subjects/\"\n",
    "gcca_dir = f\"{output_dir}/GCCA\"\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "''' Calculate dispersion of connectivity in 3D gradient space'''\n",
    "\n",
    "gcca_nw = np.load(f'{gcca_dir}/{group}.gcca.{nw_N}NWs.npy')\n",
    "gcca_parc = np.load(f'{gcca_dir}/{group}.gcca.{lbl_N}Parc.npy')\n",
    "gcca_vtx = np.load(f'{gcca_dir}/{group}.gcca.32k_fs_LR.npy')\n",
    "\n",
    "# AVVERAGE SQUARED DISTANCE from group centroid\n",
    "def gcca_dispersion(gcca):\n",
    "    centroids = gcca.mean(axis=0)\n",
    "    squares = np.square(gcca - centroids)\n",
    "    sum_squares = squares[:, :2, :].sum(axis=1)\n",
    "    distance = np.sqrt(sum_squares)\n",
    "    avg_distance2 = np.square(distance).sum(axis=0) / distance.shape[0]\n",
    "    avg_components2 = squares.sum(axis=0) / squares.shape[0]\n",
    "    return np.vstack([avg_distance2, avg_components2])\n",
    "\n",
    "dispersion_nw = gcca_dispersion(gcca_nw)\n",
    "dispersion_parc = gcca_dispersion(gcca_parc)\n",
    "dispersion_vtx = gcca_dispersion(gcca_vtx)\n",
    "\n",
    "np.save(f'{output_dir}/{group}.gcca_dispersion.{nw_N}NWs', dispersion_nw)\n",
    "np.save(f'{output_dir}/{group}.gcca_dispersion.{lbl_N}Parc', dispersion_parc)\n",
    "np.save(f'{output_dir}/{group}.gcca_dispersion.32k_fs_LR', dispersion_vtx)\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "''' Save dscalar files'''\n",
    "\n",
    "# Load gcca data\n",
    "gcca_nw = np.load(f'{gcca_dir}/{group}.gcca.{nw_N}NWs.npy')\n",
    "gcca_parc = np.load(f'{gcca_dir}/{group}.gcca.{lbl_N}Parc.npy')\n",
    "gcca_vtx = np.load(f'{gcca_dir}/{group}.gcca.32k_fs_LR.npy')\n",
    "\n",
    "# Get vertex network labels\n",
    "_, lbl_tbl = ct.agg_networks(networks, networks, by_hemisphere=False, label_tbl=True)\n",
    "vtx_nw = lbl_tbl.set_index('label').loc[networks.get_fdata().squeeze().astype('int32'), 'network'].values\n",
    "template = nib.load(f'{subj_dir}/{subj_id[0]}/Analysis/{subj_id[0]}.GCCA_525.32k_fs_LR.dscalar.nii')\n",
    "\n",
    "# Vertex - gradient median\n",
    "scalars = np.median(gcca_vtx, axis=0)\n",
    "out = f'{output_dir}/{group}.gcca.32k_fs_LR.dscalar.nii'\n",
    "ct.save_dscalar(scalars, template, out, names=['Gradient_1', 'Gradient_2', 'Gradient_3'])\n",
    "\n",
    "# Parcel - gradient median\n",
    "scalars = np.median(gcca_parc[:, :, networks.get_fdata().squeeze().astype('int32')], axis=0)\n",
    "out = f'{output_dir}/{group}.gcca.{lbl_N}Parc.32k_fs_LR.dscalar.nii'\n",
    "ct.save_dscalar(scalars, networks, out, names=['Gradient 1', 'Gradient 2', 'Gradient 3'])\n",
    "\n",
    "# Network - gradient median\n",
    "scalars = np.median(gcca_nw[:, :, vtx_nw], axis=0)\n",
    "out = f'{output_dir}/{group}.gcca.{lbl_N}Parc_{nw_N}NWs.32k_fs_LR.dscalar.nii'\n",
    "ct.save_dscalar(scalars, networks, out, names=['Gradient_1', 'Gradient_2', 'Gradient_3'])\n",
    "\n",
    "\n",
    "dispersion_nw = np.load(f'{output_dir}/{group}.gcca_dispersion.{nw_N}NWs.npy')\n",
    "dispersion_parc = np.load(f'{output_dir}/{group}.gcca_dispersion.{lbl_N}Parc.npy')\n",
    "dispersion_vtx = np.load(f'{output_dir}/{group}.gcca_dispersion.32k_fs_LR.npy')\n",
    "\n",
    "# Vertex - dispersion \n",
    "scalars = dispersion_vtx\n",
    "out = f'{output_dir}/{group}.gcca_dispersion.32k_fs_LR.dscalar.nii'\n",
    "ct.save_dscalar(scalars, template, out, names=['All', 'Gradient_1', 'Gradient_2', 'Gradient_3'])\n",
    "\n",
    "# Parcel - dispersion \n",
    "scalars = dispersion_parc[:, networks.get_fdata().squeeze().astype('int32')]\n",
    "out = f'{output_dir}/{group}.gcca_dispersion.{lbl_N}Parc.32k_fs_LR.dscalar.nii'\n",
    "ct.save_dscalar(scalars, networks, out, names=['All', 'Gradient_1', 'Gradient_2', 'Gradient_3'])\n",
    "\n",
    "# Network - dispersion \n",
    "scalars = dispersion_nw[:, vtx_nw]\n",
    "out = f'{output_dir}/{group}.gcca_dispersion.{lbl_N}Parc_{nw_N}NWs.32k_fs_LR.dscalar.nii'\n",
    "ct.save_dscalar(scalars, networks, out, names=['All', 'Gradient_1', 'Gradient_2', 'Gradient_3'])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variability_clusyters.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n",
      "/home/fralberti/Code/interindividual-gradient-dispersion/gradient-variability/settings.py:28: FutureWarning: The operation <function median at 0x7f8e003a0a60> failed on a column. If any error is raised, this will raise an exception in a future version of pandas. Drop these columns to avoid this warning.\n",
      "  nw_name = NW_tbl.groupby('name').agg(np.median).sort_values('network').index.to_list()[1:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n",
      "[[196.  67.  47. ... 339. 339. 339.]]\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import ciftools_FA as ct\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess as sp\n",
    "from joblib import Parallel, delayed\n",
    "from settings import root_dir, output_dir, subj_id, group, lbl_N, nw_N, networks, networks_txt, NW_tbl, nw_name, nj\n",
    "subj_dir = f\"{root_dir}/Subjects/\"\n",
    "gcca_dir = f\"{output_dir}/GCCA\"\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "''' Generate clusters of highest inter-individual variability '''\n",
    "\n",
    "# Find variance clusters\n",
    "dscalar = f\"{output_dir}/{group}.gcca_dispersion.32k_fs_LR.dscalar.nii\"\n",
    "percentile = 95\n",
    "clusters = []\n",
    "for g_tmp in range(4):\n",
    "    thr = np.percentile(np.load(f\"{output_dir}/{group}.gcca_dispersion.32k_fs_LR.npy\"), percentile, axis=1)[g_tmp]\n",
    "    srf_min = 200\n",
    "    vol_min = 1\n",
    "    out = f\"{output_dir}/{group}.gcca_disp_clusters.32k_fs_LR.dscalar.nii\"\n",
    "    L_srf = f\"{root_dir}/HCP_S1200_GroupAvg_v1/S1200.L.midthickness_MSMAll.32k_fs_LR.surf.gii\"\n",
    "    R_srf = f\"{root_dir}/HCP_S1200_GroupAvg_v1/S1200.R.midthickness_MSMAll.32k_fs_LR.surf.gii\"\n",
    "\n",
    "    sp.run(f\"wb_command -cifti-find-clusters {dscalar} {thr} {srf_min} {thr} {vol_min} COLUMN {out} -left-surface {L_srf} -right-surface {R_srf}\", shell=True)\n",
    "\n",
    "    clusters_g = nib.load(f\"{output_dir}/{group}.gcca_disp_clusters.32k_fs_LR.dscalar.nii\").get_fdata()[g_tmp, :]\n",
    "    clusters.append(clusters_g)\n",
    "\n",
    "clusters = np.asarray(clusters)\n",
    "clusters = stats.rankdata(clusters, axis=1, method='dense') - 1\n",
    "\n",
    "ct.save_dscalar(clusters, nib.load(dscalar), out, names=['All', 'Gradient_1', 'Gradient_2', 'Gradient_3'])\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "''' Save clusters as dlabel.nii file '''\n",
    "\n",
    "clusters = nib.load(f\"{output_dir}/{group}.gcca_disp_clusters.32k_fs_LR.dscalar.nii\")\n",
    "\n",
    "\n",
    "labels_path = f\"{output_dir}/{group}.gcca_disp_clusters.32k_fs_LR.dscalar.nii\"\n",
    "labels_array = nib.load(labels_path)\n",
    "lbl_txt =  f'{output_dir}/{group}.gcca_disp_clusters_info.txt'\n",
    "new_dlabel = f'{output_dir}/{group}.gcca_disp_clusters_info.dlabel.txt'\n",
    "\n",
    "\n",
    "labels = np.unique(clusters.get_fdata()).astype('int32')\n",
    "new_labels = []\n",
    "colors = np.round(plt.cm.Dark2(range(len(labels)-1))* 255).astype('int32').astype('str')\n",
    "for lbl in labels[labels!=0]:\n",
    "    new_labels.append(f\"Disp_cluster_{lbl}\")\n",
    "    new_labels.append(' '.join(np.hstack([lbl, colors[lbl-1,:]])))\n",
    "\n",
    "#with open(lbl_txt, 'w') as f:\n",
    "#    #lines = f.readlines()\n",
    "#    lines.extend([s + '\\n' for s in new_labels])\n",
    "#    f.close()\n",
    "\n",
    "with open(lbl_txt, 'w') as f:\n",
    "    f.write('\\n'.join(new_labels))\n",
    "    \n",
    "    \n",
    "# Create dlabel file\n",
    "new_dlabel = f'{output_dir}/{group}.gcca_disp_clusters.32k_fs_LR.dlabel.nii'\n",
    "cmd = f'wb_command -cifti-label-import {labels_path} {lbl_txt} {new_dlabel} -discard-others'\n",
    "sp.run(cmd, shell=True)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "''' Add dispersion cluster to Schaefer atlas '''\n",
    "\n",
    "clusters = nib.load(f\"{output_dir}/{group}.gcca_disp_clusters.32k_fs_LR.dscalar.nii\")\n",
    "\n",
    "# allign network and cluster maps\n",
    "shared_l = np.isin(ct.struct_info('CIFTI_STRUCTURE_CORTEX_LEFT', networks)[2], ct.struct_info('CIFTI_STRUCTURE_CORTEX_LEFT', clusters)[2])\n",
    "shared_r = np.isin(ct.struct_info('CIFTI_STRUCTURE_CORTEX_RIGHT', networks)[2], ct.struct_info('CIFTI_STRUCTURE_CORTEX_RIGHT', clusters)[2])\n",
    "shared_vv = np.hstack([shared_l, shared_r])\n",
    "\n",
    "clusters = clusters.get_fdata()[0].astype('int32')\n",
    "clusters[clusters != 0] += 1000    ### add 1000 to differentiate from original parcels\n",
    "cluster_N = len(np.unique(clusters))\n",
    "parcels = networks.get_fdata()[0, shared_vv].astype('int32')\n",
    "parcels[clusters!=0] = clusters[clusters!=0]\n",
    "removed = np.unique(networks.get_fdata()[0, shared_vv].astype('int32'))[~np.isin(np.unique(networks.get_fdata()[0, shared_vv].astype('int32')), parcels)]\n",
    "\n",
    "# save dscalar with new parcellation\n",
    "out = f'{output_dir}/{group}.dispROIs_Schaefer2018_{lbl_N}Parcels_{nw_N}Networks.dscalar.nii'\n",
    "template = nib.load(f\"{output_dir}/{group}.gcca_disp_clusters.32k_fs_LR.dscalar.nii\")\n",
    "ct.save_dscalar(parcels, template, out)\n",
    "\n",
    "\n",
    "# Create list of new parcels and colors\n",
    "new_labels = []\n",
    "names = [f\"{nw_N}Networks_LH_ROI_{i}\" for i in range(1,cluster_N)]\n",
    "colors = np.round(plt.cm.Dark2(range(np.unique(clusters[clusters!=0]).size))* 255).astype('int32').astype('str')\n",
    "for i, lbl in enumerate(np.unique(clusters[clusters!=0])):\n",
    "    new_labels.append(names[i]+'\\n')\n",
    "    new_labels.append(' '.join(np.hstack([lbl, colors[i,:], '\\n'])))\n",
    "   \n",
    "    \n",
    "# Save new label-list-file with added parcels\n",
    "orig_lbl_txt = networks_txt\n",
    "with open(orig_lbl_txt, 'r+') as f:\n",
    "    lines = f.readlines()\n",
    "    lines.extend(new_labels)\n",
    "    f.close()\n",
    "\n",
    "for i in range(1, len(lines), 2):\n",
    "    if i > len(lines):\n",
    "        break\n",
    "    label = int(lines[i].split()[0])\n",
    "    if label not in parcels:\n",
    "        del lines[i], lines[i-1]\n",
    "\n",
    "new_lbl_txt = f'{output_dir}/{group}.dispROIs_Schaefer2018_{lbl_N}Parcels_{nw_N}Networks.txt'\n",
    "with open(new_lbl_txt, 'w') as f:\n",
    "    f.truncate(0)\n",
    "    f.write(''.join(lines))\n",
    "    f.close()\n",
    "    \n",
    "# Create dlabel file\n",
    "new_dlabel = f'{output_dir}/{group}.dispROIs_Schaefer2018_{lbl_N}Parcels_{nw_N}Networks.dlabel.nii'\n",
    "cmd = f'wb_command -cifti-label-import {out} {new_lbl_txt} {new_dlabel}'\n",
    "sp.run(cmd, shell=True)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "''' Generate previous tables and images using the integrated parcellation '''\n",
    "\n",
    "joint_atlas = nib.load(f'{output_dir}/{group}.dispROIs_Schaefer2018_{lbl_N}Parcels_{nw_N}Networks.dlabel.nii')\n",
    "labels = np.unique(joint_atlas.get_fdata()).astype('int32')\n",
    "\n",
    "# Parcel - gradient median\n",
    "gcca_parc = []\n",
    "for subj in subj_id:\n",
    "    grad = nib.load(f'{subj_dir}/{subj}/Analysis/{subj}.GCCA_525.32k_fs_LR.dscalar.nii')\n",
    "    lbl_df = ct.agg_labels(grad, joint_atlas, func=np.median)\n",
    "    gcca_parc.append(lbl_df.T.values)\n",
    "    np.save(f'{output_dir}/{subj}.gcca.dispROIs_{lbl_N}Parc', lbl_df)\n",
    "    \n",
    "gcca_parc = np.array(gcca_parc)\n",
    "np.save(f'{output_dir}/{group}.gcca.dispROIs_{lbl_N}Parc', gcca_parc)\n",
    "\n",
    "parc_idx = stats.rankdata(joint_atlas.get_fdata().squeeze(), method='dense') - 1\n",
    "scalars = np.median(gcca_parc[:, :, parc_idx], axis=0)\n",
    "out = f'{output_dir}/{group}.gcca.dispROIs_{lbl_N}Parc.32k_fs_LR.dscalar.nii'\n",
    "ct.save_dscalar(scalars, joint_atlas, out, names=['Gradient 1', 'Gradient 2', 'Gradient 3'])\n",
    "\n",
    "\n",
    "# Parcel - dispersion \n",
    "dispersion_vtx = nib.load(f'{output_dir}/{group}.gcca_dispersion.32k_fs_LR.dscalar.nii')\n",
    "dispersion_parc = ct.agg_labels(dispersion_vtx, joint_atlas, func=np.median)\n",
    "np.save(f'{output_dir}/{group}.gcca_dispersion.dispROIs_{lbl_N}Parc', dispersion_parc)\n",
    "\n",
    "scalars = joint_atlas.get_fdata().copy()\n",
    "\n",
    "for lbl, val in dispersion_parc.iterrows():\n",
    "    scalars[:,scalars[0]==lbl] = val.iloc[0]\n",
    "    \n",
    "out = f'{output_dir}/{group}.gcca_dispersion.dispROIs_{lbl_N}Parc.32k_fs_LR.dscalar.nii'\n",
    "ct.save_dscalar(scalars, joint_atlas, out, names=['All'])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Vis  SomMot        DAN        VAN  Limbic        FPN  \\\n",
      "DispROI_tot                                                           \n",
      "L TPJ          0.0     0.0  18.489583  41.145833     0.0   0.000000   \n",
      "L vlPFC        0.0     0.0  18.343195  57.396450     0.0  21.301775   \n",
      "L dlPFC        0.0     0.0   0.000000  15.822785     0.0   8.860759   \n",
      "L OP         100.0     0.0   0.000000   0.000000     0.0   0.000000   \n",
      "R TPJ          0.0     0.0  29.881657  34.615385     0.0   0.000000   \n",
      "R vlPFC        0.0     0.0   1.081081  14.234234     0.0  58.738739   \n",
      "R dlPFC        0.0     0.0   0.000000   3.171247     0.0  50.528541   \n",
      "R OP         100.0     0.0   0.000000   0.000000     0.0   0.000000   \n",
      "\n",
      "                   DMN    TOT  \n",
      "DispROI_tot                    \n",
      "L TPJ        40.364583  100.0  \n",
      "L vlPFC       2.958580  100.0  \n",
      "L dlPFC      75.316456  100.0  \n",
      "L OP          0.000000  100.0  \n",
      "R TPJ        35.502959  100.0  \n",
      "R vlPFC      25.945946  100.0  \n",
      "R dlPFC      46.300211  100.0  \n",
      "R OP          0.000000  100.0  \n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import ciftools_FA as ct\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess as sp\n",
    "from joblib import Parallel, delayed\n",
    "#from settings import root_dir, output_dir, subj_id, cog_path, group, lbl_N, nw_N, networks, networks_txt, NW_tbl, nw_name, nj\n",
    "subj_dir = f\"{root_dir}/Subjects/\"\n",
    "gcca_dir = f\"{output_dir}/GCCA\"\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "''' Generate table with vertex affiliations, gradients, and dispersion measures '''\n",
    "\n",
    "grad_SD = nib.load(f'{output_dir}/{group}.gcca_dispersion.32k_fs_LR.dscalar.nii')\n",
    "cifti_59k = nib.load(f'{subj_dir}/{subj_id[0]}/Analysis/{subj_id[0]}.GCCA_525.32k_fs_LR.dscalar.nii')\n",
    "\n",
    "# Create parcel label array\n",
    "parc_lbl = []\n",
    "vv = []\n",
    "hemi = []\n",
    "for struct in ['CIFTI_STRUCTURE_CORTEX_LEFT', 'CIFTI_STRUCTURE_CORTEX_RIGHT']:\n",
    "    NWos, NWn, NWvv = ct.struct_info(struct, networks)\n",
    "    Gos, Gn, Gvv = ct.struct_info(struct, cifti_59k)\n",
    "    shared_vv = np.isin(NWvv, Gvv)\n",
    "    parc_lbl.extend(networks.get_fdata()[0, NWos : NWos + NWn][shared_vv])\n",
    "    vv.extend(NWvv[shared_vv])\n",
    "    h = (0 if struct=='CIFTI_STRUCTURE_CORTEX_LEFT' else 1)\n",
    "    hemi.extend(np.repeat(h, Gn))\n",
    "\n",
    "\n",
    "# Create network label array\n",
    "parc_to_NW = ct.agg_networks(networks, networks, func='mean', by_hemisphere=False, label_tbl=True)[1]\n",
    "NW_lbl = parc_to_NW.set_index('label').loc[parc_lbl, 'network'].values\n",
    "\n",
    "# Create dataframe for tests\n",
    "disp_df = pd.DataFrame(np.vstack([vv, hemi, parc_lbl, NW_lbl, grad_SD.get_fdata()]).T, columns = ['Vtx', 'Hemi', 'Parc', 'NW', 'Disp_tot', 'Disp_G1', 'Disp_G2', 'Disp_G3'])\n",
    "\n",
    "# Add ROIs labels\n",
    "clusters = nib.load(f\"{output_dir}/{group}.gcca_disp_clusters.32k_fs_LR.dscalar.nii\").get_fdata().T\n",
    "disp_df[['DispROI_tot', 'DispROI_G1', 'DispROI_G2', 'DispROI_G3']] = clusters\n",
    "\n",
    "\n",
    "disp_df.to_csv(f'{output_dir}/{group}.gcca_dispersion.csv', index=False)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "''' Generate table with subject data for analyses '''\n",
    "\n",
    "# Load cognitive scores\n",
    "cog_df = pd.read_csv(cog_path,\n",
    "                    usecols = ['Subject', 'PicVocab_Unadj', 'ReadEng_Unadj', 'CardSort_Unadj', 'Flanker_Unadj', 'ProcSpeed_Unadj',\n",
    "                               'VSPLOT_TC', 'PMAT24_A_CR', 'PicSeq_Unadj', 'ListSort_Unadj', 'IWRD_TOT',\n",
    "                               'CogEarlyComp_Unadj', 'CogFluidComp_Unadj', 'CogCrystalComp_Unadj', 'CogTotalComp_Unadj',\n",
    "                               'ER40_CR'],\n",
    "                     index_col = 'Subject').loc[subj_id,:]\n",
    "\n",
    "# Calculate weighted factors\n",
    "cog_df['G'] = (cog_df[['PicVocab_Unadj', 'ReadEng_Unadj', 'CardSort_Unadj', 'Flanker_Unadj', 'ProcSpeed_Unadj',\n",
    "                       'VSPLOT_TC', 'PMAT24_A_CR', 'PicSeq_Unadj', 'ListSort_Unadj', 'IWRD_TOT']]\n",
    "               * [.624, .642, .364, .259, .232, .578, .626, .354, .451, .294]).mean(axis=1)\n",
    "\n",
    "\n",
    "# Load covariates for correcting cognitive scores\n",
    "covar_df = pd.merge(pd.read_csv(cog_path,\n",
    "                                usecols=['Subject', 'Gender'],\n",
    "                                dtype={'Subject':'int32', 'Gender':'category'}),\n",
    "                    pd.read_csv(f'{root_dir}/RESTRICTED_arianna_9_7_2022_8_13_11.csv',\n",
    "                                usecols=['Subject', 'Age_in_Yrs', 'Handedness', 'SSAGA_Educ'],\n",
    "                                dtype={'Subject':'int32', 'Age_in_Yrs':'int32', 'Handedness':'int32'}),\n",
    "                    ).set_index('Subject')\n",
    "\n",
    "covar_df = covar_df.loc[subj_id]\n",
    "cog_df = cog_df.merge(covar_df, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "# Add aggregate gradient value of the ROIs\n",
    "ROIs = pd.read_csv(f'{output_dir}/{group}.gcca_dispersion.csv')\n",
    "cog_df.index = cog_df.index.astype('str')\n",
    "\n",
    "grads = np.load(f'{gcca_dir}/{group}.gcca.32k_fs_LR.npy').copy()\n",
    "\n",
    "for grd in ['tot', 'G1', 'G2', 'G3']:\n",
    "    for i in range(3):\n",
    "        grads_df = pd.DataFrame(np.vstack([ROIs[f\"DispROI_{grd}\"], grads[:, i, :]]).T, columns=np.hstack([\"ROI\", subj_id]))\n",
    "        grads_avg = grads_df.groupby(\"ROI\").agg(np.median).reset_index().T.iloc[:,1:].drop('ROI')\n",
    "        cols = [f\"G{i+1}_ROI{int(n)}_Disp{grd}\" for n in grads_avg.columns]\n",
    "        grads_avg = grads_avg.rename(columns=dict(zip(grads_avg.columns, cols))).rename_axis('Subject')\n",
    "        cog_df = cog_df.merge(grads_avg, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "\n",
    "cog_df.to_csv(f'{output_dir}/{group}.cog_data.csv')\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "''' Measure clusters' overlap with networks '''\n",
    "\n",
    "disp_df = pd.read_csv(f\"{output_dir}/{group}.gcca_dispersion.csv\", header=0, usecols=[\"Vtx\", \"NW\", f\"DispROI_tot\"])\n",
    "disp_df = disp_df[disp_df[f\"DispROI_tot\"] > 0]\n",
    "\n",
    "clusters = disp_df.groupby([f\"DispROI_tot\", \"NW\"]).agg(\"count\")[\"Vtx\"].reset_index().pivot(index=f\"DispROI_tot\", columns=\"NW\", values=\"Vtx\")\n",
    "clusters.columns = np.array(nw_name)[clusters.columns.astype(\"int32\")-1]\n",
    "clusters.loc[\"TOT\", :] = clusters.sum()\n",
    "clusters.loc[:, \"TOT\"] = clusters.sum(axis=1)\n",
    "\n",
    "clusters.to_csv(f\"{output_dir}/{group}.dispROIs_{nw_N}NWs_overlap.csv\")\n",
    "\n",
    "\n",
    "missing_NWs = np.asarray(nw_name)[~np.isin(nw_name, clusters.columns)]\n",
    "missing_NWs = pd.DataFrame(np.full([len(clusters),  len(missing_NWs)], np.nan), columns=missing_NWs, index=clusters.index)\n",
    "clusters = clusters.merge(missing_NWs, left_index=True, right_index=True)[np.hstack([nw_name, 'TOT'])]\n",
    "\n",
    "clusters = (clusters.T / clusters.T.sum() * 100).T * 2\n",
    "titles = ['L TPJ', 'L vlPFC', 'L OP', 'L dlPFC', 'R TPJ', 'R vlPFC', 'R dlPFC', 'R OP']\n",
    "\n",
    "clusters.rename(columns={'SalVentAttn':'VAN', 'DorsAttn':'DAN', 'Default':'DMN', 'Cont':'FPN'}, index=dict(zip(clusters.index, titles)), inplace=True)\n",
    "clusters = clusters.loc[['L TPJ', 'L vlPFC', 'L dlPFC', 'L OP', 'R TPJ', 'R vlPFC', 'R dlPFC', 'R OP'], :'TOT']\n",
    "clusters[clusters.isna()] = 0\n",
    "\n",
    "print(clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modeling_intelligence.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Endog         F        R2    R2_adj       p   p_adj\n",
      "0    CogFluidComp_Unadj  2.751444  0.047643  0.030327  0.0128  0.0384\n",
      "1  CogCrystalComp_Unadj  0.956891  0.017050 -0.000768  0.4624  0.4624\n",
      "2                     G  2.132093  0.037210  0.019758  0.0504  0.0756\n",
      "post-hoc:\tCogFluidComp_Unadj\n",
      "                       Coef         t        SE       p                 p_adj\n",
      "Intercept         0.429412  1.725382  0.248879  0.0772                      \n",
      "G1_ROI2_Disptot  -0.918436 -0.682881  1.344943  0.4827               0.70524\n",
      "G1_ROI4_Disptot   0.914885  0.559184  1.636106  0.5877               0.70524\n",
      "G1_ROI6_Disptot  -5.174402 -2.649952  1.952640  0.0079  0.047400000000000005\n",
      "G1_ROI7_Disptot  -0.627346 -0.255340  2.456906  0.7939                0.7939\n",
      "G1_ROI15_Disptot -1.284491 -1.037619  1.237922  0.3001                0.6002\n",
      "G1_ROI38_Disptot -2.597633 -1.246108  2.084597  0.2121                0.6002 \n",
      "\n",
      "                     rho     p_adj    h\n",
      "CardSort_Unadj -0.137122  0.029044  1.0\n",
      "Flanker_Unadj  -0.194448  0.001616  1.0\n",
      "PMAT24_A_CR     0.007759  0.887322  0.0\n",
      "PicSeq_Unadj   -0.093024  0.146188  0.0\n",
      "ListSort_Unadj -0.014081  0.887322  0.0\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import ciftools_FA as ct\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess as sp\n",
    "from joblib import Parallel, delayed\n",
    "#from settings import root_dir, output_dir, subj_id, group, lbl_N, nw_N, networks, networks_txt, NW_tbl, nw_name, nj\n",
    "subj_dir = f\"{root_dir}/Subjects/\"\n",
    "gcca_dir = f\"{output_dir}/GCCA\"\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "''' Define functions to parallelize permutation testing '''\n",
    "\n",
    "def perm_fit(model, data, perm_cols):\n",
    "    data.loc[:, perm_cols] = np.random.permutation(data.loc[:, perm_cols])\n",
    "    lm_perm = ols(model, data=data).fit()\n",
    "    return lm_perm.fvalue, lm_perm.tvalues\n",
    "\n",
    "def perm_lm(model, data, perm_n, perm_cols=None, n_jobs=1):\n",
    "    lm = ols(model, data=data).fit()\n",
    "\n",
    "    A = np.identity(len(lm.params))\n",
    "    A = A[1:,:]\n",
    "    F_results = {'Endog': lm.model.endog_names, 'F': lm.fvalue, 'R2': lm.rsquared, 'R2_adj': lm.rsquared_adj}\n",
    "    dfs = [lm.params, lm.tvalues, lm.bse]\n",
    "    t_results = pd.DataFrame(np.vstack(dfs).T, index=lm.model.exog_names, columns=['Coef', 't', 'SE'])\n",
    "    \n",
    "    F_null = []\n",
    "    t_null = []\n",
    "    data_perm = data.copy()\n",
    "    if perm_cols is None:\n",
    "        perm_cols = data.columns[data.columns.isin(lm.model.exog_names)]\n",
    "        \n",
    "    r= Parallel(n_jobs=n_jobs)(delayed(perm_fit)(model, data_perm, perm_cols) for i in range(perm_n))\n",
    "    F_null, t_null = zip(*r)\n",
    "    \n",
    "    t_p = [sum(np.abs(np.asarray(t_null).T[n]) > np.abs(t_results.t[n])) / perm_n for n in range(len(t_results))]\n",
    "    t_results['p'] = t_p\n",
    "    F_results['p'] = (sum(F_null > F_results['F']) / perm_n)\n",
    "\n",
    "    return F_results, t_results, np.array(F_null), np.array(t_null)\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "''' Run permutation tests '''\n",
    "\n",
    "# Aggregate highly correlated predictors\n",
    "cog_df = pd.read_csv(f'{output_dir}/{group}.cog_data.csv', index_col=0, header=0)\n",
    "G = 'G1'\n",
    "cog_df[f'{G}_ROI38_Disptot'] = np.mean(cog_df[[f'{G}_ROI3_Disptot', f'{G}_ROI7_Disptot']], axis=1)\n",
    "cog_df[f'{G}_ROI15_Disptot'] = np.mean(cog_df[[f'{G}_ROI1_Disptot', f'{G}_ROI5_Disptot']], axis=1)\n",
    "G = 'G2'\n",
    "cog_df[f'{G}_ROI38_Disptot'] = np.mean(cog_df[[f'{G}_ROI3_Disptot', f'{G}_ROI7_Disptot']], axis=1)\n",
    "cog_df[f'{G}_ROI15_Disptot'] = np.mean(cog_df[[f'{G}_ROI1_Disptot', f'{G}_ROI5_Disptot']], axis=1)\n",
    "G = 'G3'\n",
    "cog_df[f'{G}_ROI38_Disptot'] = np.mean(cog_df[[f'{G}_ROI3_Disptot', f'{G}_ROI7_Disptot']], axis=1)\n",
    "cog_df[f'{G}_ROI15_Disptot'] = np.mean(cog_df[[f'{G}_ROI1_Disptot', f'{G}_ROI5_Disptot']], axis=1)\n",
    "\n",
    "\n",
    "comp_cols = ['CogFluidComp_Unadj', 'CogCrystalComp_Unadj', 'G']\n",
    "ROI_cols = np.array(['G1_ROI2_Disptot', 'G1_ROI4_Disptot', 'G1_ROI6_Disptot', 'G1_ROI7_Disptot', 'G1_ROI15_Disptot', 'G1_ROI38_Disptot'])\n",
    "covars = ['C(Gender)', 'Age_in_Yrs', 'Handedness', 'SSAGA_Educ']\n",
    "\n",
    "\n",
    "# Correct for covariates\n",
    "X = ' + '.join(covars)\n",
    "for col in comp_cols:\n",
    "    lm = ols(f'{col} ~ {X}', data=cog_df).fit()\n",
    "    cog_df[col] = stats.zscore(lm.resid, axis=0)\n",
    "\n",
    "\n",
    "# Run permutation tests\n",
    "perm_n = 10000\n",
    "X = ' + '.join(ROI_cols)\n",
    "F_tests = []\n",
    "F_test_p = []\n",
    "\n",
    "results = {}\n",
    "for y in comp_cols:\n",
    "    F, t, F_null, t_null = perm_lm(f'{y} ~ {X}', cog_df, perm_n, perm_cols=None, n_jobs=-2)\n",
    "    results[y] = {'F': F, 't': t, 'F_null': F_null, 't_null': t_null}\n",
    "    \n",
    "    \n",
    "# Correct multiple comparisons\n",
    "pF_adj = sm.stats.multipletests([values['F']['p'] for key, values in results.items()], method='fdr_bh', alpha=0.05)[1]\n",
    "\n",
    "for i, key in enumerate(results.keys()):\n",
    "    results[key]['F']['p_adj'] = pF_adj[i]\n",
    "    results[key]['t']['p_adj'] = np.hstack(['',sm.stats.multipletests(results[key]['t']['p'][1:], method='fdr_bh', alpha=0.05)[1]])\n",
    "\n",
    "global_results = pd.DataFrame([results['CogFluidComp_Unadj']['F'], results['CogCrystalComp_Unadj']['F'], results['G']['F']])\n",
    "\n",
    "print(global_results)\n",
    "\n",
    "for key, value in results.items():\n",
    "    if results[key]['F']['p_adj'] < 0.05:\n",
    "        print(f\"post-hoc:\\t{key}\\n\", pd.DataFrame(results[key]['t']), '\\n')\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------\n",
    "        \n",
    "\n",
    "''' Test correlation with individual tests '''        \n",
    "        \n",
    "# test correlation between Fluid intelligence and its components\n",
    "r, p = stats.spearmanr(cog_df[['CardSort_Unadj', 'Flanker_Unadj', 'PMAT24_A_CR', 'PicSeq_Unadj', 'ListSort_Unadj', 'CogFluidComp_Unadj']], nan_policy='omit')\n",
    "\n",
    "cols = ['CardSort_Unadj', 'Flanker_Unadj', 'PMAT24_A_CR', 'PicSeq_Unadj', 'ListSort_Unadj', 'CogFluidComp_Unadj', 'G1_ROI6_Disptot']\n",
    "r, p = stats.spearmanr(cog_df[cols], nan_policy='omit')\n",
    "r = pd.DataFrame(r, index=cols, columns=cols)\n",
    "p = pd.DataFrame(p, index=cols, columns=cols)\n",
    "\n",
    "h, pAdj, _, _ = sm.stats.multipletests(p.loc['G1_ROI6_Disptot', 'CardSort_Unadj':'ListSort_Unadj'], method='fdr_bh')\n",
    "\n",
    "results = pd.DataFrame(np.vstack([r.loc['G1_ROI6_Disptot', 'CardSort_Unadj':'ListSort_Unadj'], pAdj, h]).T,\n",
    "             columns=['rho', 'p_adj', 'h'], index=['CardSort_Unadj', 'Flanker_Unadj', 'PMAT24_A_CR', 'PicSeq_Unadj', 'ListSort_Unadj'])\n",
    "\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graph_analysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n",
      "/home/fralberti/Code/interindividual-gradient-dispersion/gradient-variability/settings.py:28: FutureWarning: The operation <function median at 0x7fe430132a60> failed on a column. If any error is raised, this will raise an exception in a future version of pandas. Drop these columns to avoid this warning.\n",
      "  nw_name = NW_tbl.groupby('name').agg(np.median).sort_values('network').index.to_list()[1:]\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import ciftools_FA as ct\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess as sp\n",
    "from joblib import Parallel, delayed\n",
    "import networkx as nx\n",
    "from settings import root_dir, output_dir, subj_id, group, lbl_N, nw_N, networks, networks_txt, NW_tbl, nw_name, nj\n",
    "subj_dir = f\"{root_dir}/Subjects/\"\n",
    "gcca_dir = f\"{output_dir}/GCCA\"\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "''' Generate FC graphs and extract topological metrics '''\n",
    "\n",
    "thr = 90\n",
    "\n",
    "NWs_n_ROIs = nib.load(f'{output_dir}/{group}.dispROIs_Schaefer2018_{lbl_N}Parcels_{nw_N}Networks.dlabel.nii')\n",
    "\n",
    "modules_df = ct.agg_networks(NWs_n_ROIs, NWs_n_ROIs)[1][1:].reset_index()\n",
    "\n",
    "communities = [set(modules_df.label.astype('int32')[modules_df.network==nw].values) for nw in modules_df.network.unique()]\n",
    "nodes = dict( zip( range( len(modules_df.index) ), modules_df.label) )\n",
    "\n",
    "def graph_diagnostics(subj, root_dir, communities, thr):\n",
    "    M = np.genfromtxt(f'{subj_dir}/{subj}/Analysis/{subj}.REST_All_fcMatrix.csv', delimiter=',')[1:, 1:]\n",
    "    G = nx.from_numpy_matrix(M)\n",
    "    G.threshold(thr)\n",
    "    G = nx.relabel_nodes(G, nodes)\n",
    "    G = nx.algorithms.full_diagnostics(G, modules=communities, swi=False)\n",
    "    nx.write_gpickle(G,f'{subj_dir}/{subj}/Analysis/{subj}.rfMRI_graph_{thr}.Schaefer_400parcs.gpickle')\n",
    "    return [subj]\n",
    "\n",
    "_ = Parallel(n_jobs=nj, prefer='processes')(delayed(graph_diagnostics)(subj, root_dir, communities, thr) for subj in subj_id)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "''' Save graph metrics of interest to cifti '''\n",
    "\n",
    "attributes = ['strength', 'clustering', 'global_e', 'local_e', 'between_c', 'participation_c']\n",
    "\n",
    "def get_graph(subj):\n",
    "    return nx.read_gpickle(f'{subj_dir}/{subj}/Analysis/{subj}.rfMRI_graph_{thr}.Schaefer_400parcs.gpickle')\n",
    "\n",
    "individual_maps = []\n",
    "for subj in subj_id:\n",
    "    G = get_graph(subj)\n",
    "    metrics = np.array([list(nx.get_node_attributes(G, attribute).values()) for attribute in attributes])\n",
    "    \n",
    "    isolates = nx.isolates(G)\n",
    "    metrics[:,np.isin(G, list(isolates))] = np.nan\n",
    "    individual_maps.append(metrics)\n",
    "    \n",
    "    \n",
    "individual_maps = np.array(individual_maps)\n",
    "np.save(f'{output_dir}/{group}.graph_metrics.dispROIs_{lbl_N}Parc', individual_maps)\n",
    "\n",
    "graph_dispersion_maps = stats.iqr(individual_maps, 0, rng=(25, 75), nan_policy='omit') \n",
    "graph_dispersion_df = pd.DataFrame(graph_dispersion_maps.T, columns=attributes)\n",
    "graph_dispersion_df.to_csv(f'{output_dir}/{group}.graph_metrics_dispersion.IQR_Schaefer2018_{lbl_N}Parc_{nw_N}NW.csv', index=False)\n",
    "\n",
    "\n",
    "joint_atlas = nib.load(f'{output_dir}/{group}.dispROIs_Schaefer2018_{lbl_N}Parcels_{nw_N}Networks.dlabel.nii')\n",
    "labels = joint_atlas.get_fdata().copy().squeeze().astype('int32')\n",
    "label_idx = stats.rankdata(labels, method='dense').squeeze() -1 \n",
    "\n",
    "\n",
    "scalars = np.zeros([graph_dispersion_df.shape[1], labels.size]).T\n",
    "\n",
    "for lbl, val in graph_dispersion_df.iterrows():\n",
    "    scalars[label_idx==lbl+1, :] = val.values\n",
    "\n",
    "out = f'{output_dir}/{group}.graph_dispersion.dispROIs_{lbl_N}Parc.32k_fs_LR.dscalar.nii'\n",
    "ct.save_dscalar(scalars.T, joint_atlas, out, names=attributes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metric_maps.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        r             p         p_adj\n",
      "strength        -0.112497  2.339245e-02  2.807094e-02\n",
      "clustering      -0.474630  2.041424e-23  1.224854e-22\n",
      "global_e         0.031853  5.221680e-01  5.221680e-01\n",
      "local_e         -0.353881  5.236228e-13  1.570868e-12\n",
      "between_c        0.173850  4.859970e-04  7.289954e-04\n",
      "participation_c -0.250144  4.914576e-07  9.829153e-07\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import ciftools_FA as ct\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess as sp\n",
    "from joblib import Parallel, delayed\n",
    "import networkx as nx\n",
    "from settings import root_dir, output_dir, subj_id, group, lbl_N, nw_N, networks, networks_txt, NW_tbl, nw_name, nj\n",
    "subj_dir = f\"{root_dir}/Subjects/\"\n",
    "gcca_dir = f\"{output_dir}/GCCA\"\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "''' Test correlation between dispersion of graph metrics and principal gradient '''\n",
    "\n",
    "gcca_dispersion = np.load(f'{output_dir}/{group}.gcca_dispersion.dispROIs_{lbl_N}Parc.npy')[:,1:]\n",
    "gcca_median = np.median(np.load(f'{output_dir}/{group}.gcca.dispROIs_{lbl_N}Parc.npy')[:,0,1:].squeeze(), axis=0)\n",
    "graph_dispersion_df = pd.read_csv(f'{output_dir}/{group}.graph_metrics_dispersion.IQR_Schaefer2018_{lbl_N}Parc_{nw_N}NW.csv')\n",
    "\n",
    "graph_dispersion_df['gcca_disp'] = gcca_dispersion[1:,0]\n",
    "graph_dispersion_df['gcca'] = gcca_median\n",
    "graph_dispersion_df[graph_dispersion_df==0] = np.nan\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "for i, graph_dispersion in enumerate(graph_dispersion_df.T.iterrows()):\n",
    "    if i == 6:\n",
    "        break\n",
    "    x = graph_dispersion_df['gcca_disp']\n",
    "    y = graph_dispersion[1]\n",
    "    spearman_r = stats.spearmanr(x, y, nan_policy='omit')\n",
    "    \n",
    "    results[graph_dispersion[0]] = {'r': spearman_r[0],\n",
    "                                    'p': spearman_r[1]}\n",
    "\n",
    "p_unadj = [value['p'] for key, value in results.items()]\n",
    "p_adj = sm.stats.multipletests(p_unadj, method='fdr_bh')[1]\n",
    "\n",
    "for i, key in enumerate(results):\n",
    "    results[key]['p_adj'] = p_adj[i]\n",
    "    \n",
    "print(pd.DataFrame(results).T)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "''' Test cross-subject correlation between graph metrics and principal gradient '''\n",
    "\n",
    "attributes = ['strength', 'clustering', 'global_e', 'local_e', 'between_c', 'participation_c']\n",
    "\n",
    "\n",
    "gcca = np.load(f'{output_dir}/{group}.gcca.dispROIs_{lbl_N}Parc.npy')[:,:,1:]\n",
    "gmetrics = np.load(f'{output_dir}/{group}.graph_metrics.dispROIs_{lbl_N}Parc.npy')\n",
    "\n",
    "results = {attribute:{'r':[], 'p':[]} for attribute in attributes}\n",
    "\n",
    "for i, parc_metrics in enumerate(gmetrics[:, :, :].T):\n",
    "    for j, metric in enumerate(parc_metrics):\n",
    "        r, p = stats.spearmanr(metric, gcca[:,0,i], nan_policy='omit')\n",
    "        results[attributes[j]]['r'].extend(np.array([r]))\n",
    "        results[attributes[j]]['p'].extend(np.array([p]))\n",
    "        \n",
    "for metric, dictionary in results.items():\n",
    "    h, p, _, _ = sm.stats.multipletests(dictionary['p'], alpha=0.05, method='fdr_by')\n",
    "    dictionary['p'] = p\n",
    "    dictionary['r'] = dictionary['r']\n",
    "    \n",
    "scalars = [results[metric][index] for metric in results.keys() for index in results[metric].keys()]\n",
    "scalars = np.array(scalars)\n",
    "\n",
    "\n",
    "joint_atlas = nib.load(f'{output_dir}/{group}.dispROIs_Schaefer2018_{lbl_N}Parcels_{nw_N}Networks.dlabel.nii')\n",
    "labels = joint_atlas.get_fdata().copy().squeeze().astype('int32')\n",
    "label_idx = stats.rankdata(labels, method='dense').squeeze() -1 \n",
    "\n",
    "maps_array = np.zeros([scalars.shape[0], labels.size])\n",
    "\n",
    "for i, scalar in enumerate(scalars):\n",
    "    for lbl in np.unique(label_idx):\n",
    "        if lbl==0:\n",
    "            continue\n",
    "        maps_array[i, label_idx==lbl] = scalar[lbl-1]\n",
    "    \n",
    "out = f'{output_dir}/{group}.graph_metric_correlation.dispROIs_{lbl_N}Parc.32k_fs_LR.dscalar.nii'\n",
    "names = np.asanyarray([[f'{metric}_R', f'{metric}_pval'] for metric in results]).reshape(-1).tolist()\n",
    "ct.save_dscalar(maps_array, joint_atlas, out, names=names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcca = np.load(f'{output_dir}/{group}.gcca.dispROIs_{lbl_N}Parc.npy')[:,:,1:]\n",
    "gmetrics = np.load(f'{output_dir}/{group}.graph_metrics.dispROIs_{lbl_N}Parc.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
